{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Jeehyun\\ECE6780\\Data\\vgg\n"
     ]
    }
   ],
   "source": [
    "cd C:/Users/PC/Jeehyun/ECE6780/Data/vgg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import model_from_json\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_conn(mag):\n",
    "    train_dir = 'breast_' + mag + '_split/train'\n",
    "    val_dir = 'breast_' + mag + '_split/val'\n",
    "    test_dir = 'breast_' + mag + '_split/test'\n",
    "\n",
    "    if mag == '40':\n",
    "        b_train, m_train, b_test, m_test, b_val, m_val = 500, 1170, 100, 137, 137, 174\n",
    "    if mag == '100':\n",
    "        b_train, m_train, b_test, m_test, b_val, m_val = 515, 1237, 100, 145, 135, 188\n",
    "    if mag == '200':\n",
    "        b_train, m_train, b_test, m_test, b_val, m_val = 498, 1190, 100, 139, 137, 178\n",
    "    if mag == '400':\n",
    "        b_train, m_train, b_test, m_test, b_val, m_val = 470, 1032, 100, 124, 140, 147        \n",
    "    \n",
    "    nb_train =  b_train + m_train\n",
    "    nb_val =  b_val + m_val\n",
    "    nb_test =  b_test + m_test\n",
    "\n",
    "    train_labels = np.array([0] * b_train + [1] * m_train)\n",
    "    val_labels = np.array([0] * b_val + [1] * m_val)\n",
    "    test_labels = np.array([0] * b_test + [1] * m_test)\n",
    "    return train_dir, val_dir, test_dir, nb_train, nb_val, nb_test, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_extract():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "        \n",
    "    tuned_model_new_name = 'tuned_TL_' + mag_fact\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(tuned_model_new_name + '.json', 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    tuned_model = model_from_json(model_json)\n",
    "    # load weights into new model\n",
    "    tuned_model.load_weights(tuned_model_new_name + '.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    tuned_model.compile(optimizer='sgd',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    intermediate_from_model = tuned_model.get_layer(\"block5_pool\").output\n",
    "    bottleneck_model = Model(tuned_model.input, intermediate_from_model)\n",
    "    bottleneck_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    fine_bottleneck_feat_train = bottleneck_model.predict_generator(train_generator, nb_train//batch_size)\n",
    "    np.save(open('fine_bottleneck_feat_train_' + mag_fact +'.npy', 'wb'), fine_bottleneck_feat_train)\n",
    "    print(\"train extracted\")\n",
    "\n",
    "    fine_bottleneck_feat_val = bottleneck_model.predict_generator(validation_generator, nb_val//batch_size)\n",
    "    np.save(open('fine_bottleneck_feat_val_' + mag_fact +'.npy', 'wb'), fine_bottleneck_feat_val)\n",
    "    print(\"validation extracted\")\n",
    "\n",
    "    fine_bottleneck_feat_test = bottleneck_model.predict_generator(test_generator, nb_test//batch_size)\n",
    "    np.save(open('fine_bottleneck_feat_test_' + mag_fact +'.npy', 'wb'), fine_bottleneck_feat_test)\n",
    "    print(\"test extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BACH_fine_extract(mag):\n",
    "    bach_b_test = 200\n",
    "    bach_m_test = 200\n",
    "    nb_bach_test =  bach_b_test + bach_m_test\n",
    "\n",
    "    model_name = 'tuned_TL_' + mag\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(model_name + '.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    model.compile(optimizer='sgd',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    intermediate_from_model = model.get_layer(\"block5_pool\").output\n",
    "    bottleneck_model = Model(model.input, intermediate_from_model)\n",
    "    bottleneck_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    bach_test_dir = 'BACH/'\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        bach_test_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "    fine_bottleneck_feat_bach = bottleneck_model.predict_generator(\n",
    "        test_generator, nb_bach_test//batch_size)\n",
    "    np.save(open('fine_bottleneck_feat_bach_' + mag +'.npy', 'wb'), fine_bottleneck_feat_bach)\n",
    "    print(\"BACH extracted\")\n",
    "\n",
    "    score = model.evaluate_generator(test_generator,test_generator.samples//batch_size)\n",
    "    print('\\n', 'BACH Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1670 images belonging to 2 classes.\n",
      "Found 311 images belonging to 2 classes.\n",
      "Found 237 images belonging to 2 classes.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '40'\n",
    "batch_size = 1\n",
    "train_dir, val_dir, test_dir, nb_train, nb_val, nb_test, train_labels, val_labels, test_labels = mag_conn(mag_fact)\n",
    "fine_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1752 images belonging to 2 classes.\n",
      "Found 323 images belonging to 2 classes.\n",
      "Found 245 images belonging to 2 classes.\n",
      "Loaded model from disk\n",
      "train extracted\n",
      "validation extracted\n",
      "test extracted\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '100'\n",
    "batch_size = 1\n",
    "train_dir, val_dir, test_dir, nb_train, nb_val, nb_test, train_labels, val_labels, test_labels = mag_conn(mag_fact)\n",
    "fine_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1688 images belonging to 2 classes.\n",
      "Found 315 images belonging to 2 classes.\n",
      "Found 239 images belonging to 2 classes.\n",
      "Loaded model from disk\n",
      "train extracted\n",
      "validation extracted\n",
      "test extracted\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '200'\n",
    "batch_size = 1\n",
    "train_dir, val_dir, test_dir, nb_train, nb_val, nb_test, train_labels, val_labels, test_labels = mag_conn(mag_fact)\n",
    "fine_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 images belonging to 2 classes.\n",
      "Found 287 images belonging to 2 classes.\n",
      "Found 224 images belonging to 2 classes.\n",
      "Loaded model from disk\n",
      "train extracted\n",
      "validation extracted\n",
      "test extracted\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '400'\n",
    "batch_size = 1\n",
    "train_dir, val_dir, test_dir, nb_train, nb_val, nb_test, train_labels, val_labels, test_labels = mag_conn(mag_fact)\n",
    "fine_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 400 images belonging to 2 classes.\n",
      "BACH extracted\n",
      "\n",
      " BACH Test accuracy: 0.4925000071525574\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '40'\n",
    "BACH_fine_extract(mag_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 400 images belonging to 2 classes.\n",
      "BACH extracted\n",
      "\n",
      " BACH Test accuracy: 0.5024999976158142\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '100'\n",
    "BACH_fine_extract(mag_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 400 images belonging to 2 classes.\n",
      "BACH extracted\n",
      "\n",
      " BACH Test accuracy: 0.5099999904632568\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '200'\n",
    "BACH_fine_extract(mag_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Found 400 images belonging to 2 classes.\n",
      "BACH extracted\n",
      "\n",
      " BACH Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "mag_fact = '400'\n",
    "BACH_fine_extract(mag_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
