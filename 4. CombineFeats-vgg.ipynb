{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Jeehyun\\ECE6780\\Data\\vgg\n"
     ]
    }
   ],
   "source": [
    "cd C:/Users/PC/Jeehyun/ECE6780/Data/vgg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Lambda, Conv2DTranspose\n",
    "from keras.layers import Input\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeDIr = 'C:/Users/PC/Jeehyun/ECE6780/Data/vgg/'\n",
    "b_train = 498\n",
    "m_train = 1190\n",
    "b_test = 100\n",
    "m_test = 139\n",
    "b_val = 137\n",
    "m_val = 178\n",
    "nb_train_samples =  b_train + m_train\n",
    "nb_validation_samples =  b_val + m_val\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_fact = '200'\n",
    "train_dir = 'breast_' + mag_fact + '_split/train'\n",
    "val_dir = 'breast_' + mag_fact + '_split/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 252, 252, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1016064)           0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 1016065   \n",
      "=================================================================\n",
      "Total params: 1,034,881\n",
      "Trainable params: 1,034,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################################################################\n",
    "# Nuclei_feature\n",
    "############################################################################################\n",
    "nuc_dir = 'C:/Users/PC/Jeehyun/ECE6780/Data/nuc_Feature/'\n",
    "\n",
    "nuc_file_name = nuc_dir+ 'breakhis_mp_nuclei_feature_' + mag_fact + '.mat'\n",
    "nuc_data = []\n",
    "with h5py.File(nuc_file_name) as f:\n",
    "    for column in f['breakhis_mp_nuclei_feature']:\n",
    "        row_data = []\n",
    "        for row_number in range(len(column)):            \n",
    "            row_data.append(f[column[row_number]][:])   \n",
    "        nuc_data.append(row_data)\n",
    "        \n",
    "nuc_file_name = nuc_dir+ 'breakhis_mp_nuclei_feature_' + mag_fact + '_val.mat'\n",
    "nuc_data_val = []\n",
    "with h5py.File(nuc_file_name) as f:\n",
    "    for column in f['breakhis_mp_nuclei_feature']:\n",
    "        row_data = []\n",
    "        for row_number in range(len(column)):            \n",
    "            row_data.append(f[column[row_number]][:])   \n",
    "        nuc_data_val.append(row_data)        \n",
    "\n",
    "Nuc_feat = np.array(nuc_data)\n",
    "Nuc_val_feat = np.array(nuc_data_val)\n",
    "del nuc_data, nuc_data_val\n",
    "\n",
    "Nuc_feat = Nuc_feat.reshape(len(Nuc_feat), 256,256,1)\n",
    "Nuc_val_feat = Nuc_val_feat.reshape(len(Nuc_val_feat), 256,256,1)\n",
    "\n",
    "############################################################################################\n",
    "# Cascaded PCA\n",
    "############################################################################################\n",
    "pca_dir = 'C:/Users/PC/Jeehyun/ECE6780/Data/PCA_Feature/'\n",
    "\n",
    "pca_file_name = pca_dir+ 'breakhis_pca_feature_' + mag_fact + '.mat'\n",
    "pca_data = []\n",
    "with h5py.File(pca_file_name) as f:\n",
    "    for column in f['breakhis_pca_feature']:\n",
    "        pca_data.append(column)\n",
    "        \n",
    "pca_file_name = pca_dir+ 'breakhis_pca_feature_' + mag_fact + '_val.mat'\n",
    "pca_data_val = []\n",
    "with h5py.File(pca_file_name) as f:\n",
    "    for column in f['breakhis_pca_feature']:\n",
    "        pca_data_val.append(column)\n",
    "pca_data = np.array(pca_data)\n",
    "pca_data_val = np.array(pca_data_val)\n",
    "\n",
    "############################################################################################\n",
    "# CNN feature\n",
    "############################################################################################\n",
    "vgg_train = np.load('fine_bottleneck_feat_train.npy')\n",
    "vgg_val = np.load('fine_bottleneck_feat_val.npy')\n",
    "vgg_train_reshape = vgg_train.reshape(1688, 7*7*512)\n",
    "vgg_val_reshape = vgg_val.reshape(315, 7*7*512)\n",
    "\n",
    "\n",
    "Nuc_feat_n = Nuc_feat/((Nuc_feat-Nuc_feat.min()).max())\n",
    "pca_data_n = pca_data/((pca_data-pca_data.min()).max())\n",
    "vgg_train_n = vgg_train_reshape/((vgg_train_reshape-vgg_train_reshape.min()).max())\n",
    "\n",
    "Nuc_val_feat_n = Nuc_val_feat/((Nuc_val_feat-Nuc_val_feat.min()).max())\n",
    "pca_data_val_n = pca_data_val/((pca_data_val-pca_data_val.min()).max())\n",
    "vgg_val_n = vgg_val_reshape/((vgg_val_reshape-vgg_val_reshape.min()).max())\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "# Input 1\n",
    "############################################################################################\n",
    "model_1 = Sequential()\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape= (256, 256, 1)))\n",
    "model_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Flatten())\n",
    "#model_1.add(Dense(128, activation='relu'))\n",
    "#model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "# Input 2\n",
    "############################################################################################\n",
    "PCA_input = Input(shape=(12,))\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(6, input_dim=12))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(Dropout(0.25))\n",
    "model_3.add(Dense(1))\n",
    "model_3.add(Activation('sigmoid'))\n",
    "model_3.compile(optimizer='sgd',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "# Input 3\n",
    "############################################################################################\n",
    "vgg_input = Input(shape=(vgg_train.shape[1:]))\n",
    "vgg_re_input = Input(shape=(vgg_train_reshape.shape[1:]))\n",
    "\n",
    "############################################################################################\n",
    "# Merging\n",
    "############################################################################################\n",
    "merge_layer = concatenate([model_1.output, model_3.output, vgg_re_input])\n",
    "dnn_layer = Dense(128, activation=\"relu\")(merge_layer)\n",
    "dnn_output = Dense(1, activation=\"sigmoid\")(dnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = Model(inputs=[model_1.input, model_3.input, vgg_re_input], outputs=dnn_output)\n",
    "model_cat.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1688 samples, validate on 315 samples\n",
      "Epoch 1/1\n",
      "1688/1688 [==============================] - 155s 92ms/step - loss: 0.6915 - accuracy: 0.5865 - val_loss: 0.6918 - val_accuracy: 0.5683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ddae548688>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs=1\n",
    "\n",
    "# Nuc_feat_n\n",
    "# pca_data_n\n",
    "# vgg_train_n\n",
    "# Nuc_val_feat_n\n",
    "# pca_data_val_n\n",
    "# vgg_val_n\n",
    "\n",
    "model_cat.fit(x = [Nuc_feat_n, pca_data_n, vgg_train_n], y = train_labels,\n",
    "             validation_data = ([Nuc_val_feat_n, pca_data_val_n, vgg_val_n], val_labels),\n",
    "              batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1688 samples, validate on 315 samples\n",
      "Epoch 1/1\n",
      "1688/1688 [==============================] - 158s 93ms/step - loss: 0.6056 - accuracy: 0.7050 - val_loss: 0.7390 - val_accuracy: 0.5651\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "epochs=1\n",
    "model_cat.fit(x = [Nuc_feat_n, pca_data_n, vgg_train_n], y = train_labels,\n",
    "             validation_data = ([Nuc_val_feat_n, pca_data_val_n, vgg_val_n], val_labels),\n",
    "              batch_size=batch_size, epochs=epochs, shuffle=True)\n",
    "\n",
    "model_new_name = 'Final_concat_Model_' + mag_fact\n",
    "model_json = model_cat.to_json()\n",
    "with open(model_new_name + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model_cat.save_weights(model_new_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Merging\n",
    "############################################################################################\n",
    "merge_layer = concatenate([model_1.output,  vgg_re_input])\n",
    "dnn_layer = Dense(128, activation=\"relu\")(merge_layer)\n",
    "dnn_output = Dense(1, activation=\"sigmoid\")(dnn_layer)\n",
    "model_cat = Model(inputs=[model_1.input, vgg_re_input], outputs=dnn_output)\n",
    "model_cat.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1688 samples, validate on 315 samples\n",
      "Epoch 1/1\n",
      "1688/1688 [==============================] - 160s 95ms/step - loss: 0.6374 - accuracy: 0.6730 - val_loss: 0.7409 - val_accuracy: 0.5651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1df8def9dc8>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 24\n",
    "epochs=1\n",
    "model_cat.fit(x = [Nuc_feat, vgg_train_reshape], y = train_labels,\n",
    "             validation_data = ([Nuc_val_feat, vgg_val_reshape], val_labels),\n",
    "              batch_size=batch_size, epochs=epochs, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.434698\n",
      "-0.604897748996685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(vgg_train_reshape.max())\n",
    "print(Nuc_feat.min())\n",
    "\n",
    "Nuc_feat_n = Nuc_feat/vgg_train_reshape.max()\n",
    "#pca_data_n = pca_data/((pca_data-pca_data.min()).max())\n",
    "vgg_train_n = vgg_train_reshape/vgg_train_reshape.max()\n",
    "\n",
    "Nuc_val_feat_n = Nuc_val_feat/vgg_train_reshape.max()\n",
    "#pca_data_val_n = pca_data_val/((pca_data_val-pca_data_val.min()).max())\n",
    "vgg_val_n = vgg_val_reshape/vgg_train_reshape.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1688 samples, validate on 315 samples\n",
      "Epoch 1/3\n",
      "1688/1688 [==============================] - 159s 94ms/step - loss: 0.5961 - accuracy: 0.7068 - val_loss: 0.7684 - val_accuracy: 0.5683\n",
      "Epoch 2/3\n",
      "1688/1688 [==============================] - 159s 94ms/step - loss: 0.5812 - accuracy: 0.7085 - val_loss: 0.7741 - val_accuracy: 0.5651\n",
      "Epoch 3/3\n",
      "1688/1688 [==============================] - 159s 94ms/step - loss: 0.5779 - accuracy: 0.7091 - val_loss: 0.7682 - val_accuracy: 0.5683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ddb9998f08>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs=3\n",
    "model_cat.fit(x = [Nuc_feat, vgg_train_reshape], y = train_labels,\n",
    "             validation_data = ([Nuc_val_feat, vgg_val_reshape], val_labels),\n",
    "              batch_size=batch_size, epochs=epochs, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.        , -0.        , -0.        , ..., -0.        ,\n",
       "         0.03054723,  0.01296801],\n",
       "       [-0.        , -0.        , -0.        , ..., -0.        ,\n",
       "         0.02482537, -0.        ],\n",
       "       [-0.        , -0.        ,  0.05123122, ...,  0.01144865,\n",
       "         0.01608983, -0.        ],\n",
       "       ...,\n",
       "       [-0.        , -0.        ,  0.02423421, ..., -0.        ,\n",
       "         0.02153904, -0.        ],\n",
       "       [-0.        , -0.        ,  0.01688414, ..., -0.        ,\n",
       "        -0.        , -0.        ],\n",
       "       [ 0.00627513, -0.        , -0.        , ..., -0.        ,\n",
       "         0.01011185, -0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new_name = 'Final_2_concat_Model_' + mag_fact\n",
    "model_json = model_cat.to_json()\n",
    "with open(model_new_name + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model_cat.save_weights(model_new_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
